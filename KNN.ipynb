{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_1168/3494703561.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  small_data_19 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__19.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_1168/3494703561.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  large_data_6 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__6.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_1168/3494703561.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  small_data_21 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__21.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_1168/3494703561.py:19: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  large_data_13 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__13.txt', sep='  ', header=None)\n"
     ]
    }
   ],
   "source": [
    "### Sample Files\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "small_data_19 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__19.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "small_data_19 = small_data_19.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "large_data_6 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__6.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "large_data_6 = large_data_6.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Assigned Files\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "small_data_21 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__21.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "small_data_21 = small_data_21.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "large_data_13 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__13.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "large_data_13 = large_data_13.rename(columns={0 : \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier:\n",
    "    def __init__(self, train_set=None, test_set=None, k=None, nearest_neighbors=None):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.k = k\n",
    "        self.nearest_neightbors = nearest_neighbors\n",
    "\n",
    "    def distance(self, nums):\n",
    "        sum = 0\n",
    "        for num in nums:\n",
    "            sum += num**2\n",
    "        return np.sqrt(sum)\n",
    "        \n",
    "    def pred_class(self):\n",
    "        pred_classes = []\n",
    "        for row in range(self.k):\n",
    "            pred_classes.append(self.train_set[self.nearest_neighbors[row]][0])\n",
    "        ### Referenced NumPy unique to return the predicted class https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
    "        classes, counts = np.unique(pred_classes, return_counts=True)\n",
    "        if len(counts) == 1:\n",
    "            return classes\n",
    "        if counts[0] > counts[1]:\n",
    "            return classes[0]\n",
    "        return classes[1]\n",
    "    \n",
    "    def train(self):\n",
    "        ### Test every testing entry to training entry\n",
    "        nearest_neighbors = []\n",
    "        for train_row in self.train_set:\n",
    "            nearest_neighbors.append(self.distance(train_row[1:] - self.test_set[1:]))\n",
    "        ### Referenced NumPy argsort to sort by index of distances https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "        self.nearest_neighbors = np.argsort(nearest_neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start by splitting my data into train and test set\n",
    "data = {'label': [2.0000000e+000, 1.0000000e+000, 2.0000000e+000, 1.0000000e+000, 2],\n",
    "        'feature_1': [1.2340000e+010, 6.0668000e+000, 2.3400000e+010, 4.5645400e+010, 1],\n",
    "        'feature_2': [ 2.3440000e+000, 5.0770000e+000, 3.6460000e+000, 3.0045000e+000, 2]}\n",
    "data = {'label': [2, 1, 2, 1, 2],\n",
    "        'feature_1': [1, 10, 2, 11, 1],\n",
    "        'feature_2': [ 2, 11, 3, 12, 2]}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "train_data = df.sample(frac = 0.8, random_state = 4)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "# print(train_data)\n",
    "test_data = df.drop(train_data.index)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "# print(test_data)\n",
    "\n",
    "# knn = KNN_Classifier(train_data.to_numpy(), test_data.to_numpy(), 3)\n",
    "# knn.train()\n",
    "# print(knn.pred_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Selection:\n",
    "    def __init__(self, dataset=None, k=None):\n",
    "        self.dataset=dataset\n",
    "        self.dataset_backwards=copy.deepcopy(self.dataset)\n",
    "        self.k=k\n",
    "        self.feature_set = []\n",
    "        self.feature_set_backwards = []\n",
    "        self.feature_subset = pd.DataFrame(dataset['label'])\n",
    "        self.feature_subset_backwards = copy.deepcopy(self.dataset)\n",
    "        self.best_features = [-1, -1]\n",
    "    \n",
    "    def print_best_features(self):\n",
    "        print(\"Finished search!!! The best feature subset is {\" + self.best_features[0] + \"}, which has an accuracy of \" + str(self.best_features[1]*100) + \"%\")\n",
    "\n",
    "    def greedy_forward(self):\n",
    "        if self.dataset.shape[1] == 1:\n",
    "            return\n",
    "        precision = []\n",
    "        feature_name = []\n",
    "        ### Retreiving the Columns for Feature Selection\n",
    "        for column in self.dataset.columns:\n",
    "            if column == 'label':\n",
    "                continue\n",
    "            feature = pd.concat([self.feature_subset, pd.DataFrame(self.dataset[column])], axis=1).to_numpy()\n",
    "            correct = 0\n",
    "            ### K-Fold CV\n",
    "            for cfv in range(int(len(feature)/self.k)):\n",
    "                train_data = np.delete(feature, cfv, axis=0)\n",
    "                test_data = feature[cfv]\n",
    "                \n",
    "                knn = KNN_Classifier(train_data, test_data, 1)\n",
    "                knn.train()\n",
    "                if knn.pred_class()[0] == test_data[0]:\n",
    "                    correct += 1\n",
    "            precision.append(correct/int(len(feature)/self.k))\n",
    "            feature_name.append(column)\n",
    "            \n",
    "            if self.feature_set:\n",
    "                feat_str = ','.join(str(feature) for feature in self.feature_set)\n",
    "                print(\"\\tUsing feature(s) {\" + feat_str + \",\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)*100) + '%')\n",
    "            else:\n",
    "                print(\"\\tUsing feature(s) {\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)*100) + '%')\n",
    "                \n",
    "        precision_index = np.argsort(precision)\n",
    "        self.feature_set.append(feature_name[precision_index[-1]])\n",
    "        feat_str = ','.join(str(feature) for feature in self.feature_set)\n",
    "        prev = self.best_features[1]\n",
    "        self.best_features[1] = max(self.best_features[1], precision[precision_index[-1]])\n",
    "        if (self.best_features[1] != prev):\n",
    "            self.best_features[0] = feat_str\n",
    "        if precision[precision_index[-1]] < self.best_features[1]:\n",
    "            print(\"\\n(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\")\n",
    "        print(\"\\nFeature set {\" + feat_str + \"} was best, accuracy is \" + str(precision[precision_index[-1]]*100) + \"%\\n\")\n",
    "        self.feature_subset = pd.concat([self.feature_subset, pd.DataFrame(self.dataset[feature_name[precision_index[-1]]])], axis=1)\n",
    "        ### Referenced Pandas library to understand how to drop column https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "        self.dataset = self.dataset.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        self.greedy_forward()\n",
    "        pass\n",
    "    \n",
    "    def greedy_backward(self):\n",
    "        if self.feature_subset_backwards.shape[1] == 1:\n",
    "            return\n",
    "        precision = []\n",
    "        feature_name = []\n",
    "        ### Retreiving the Columns for Feature Selection\n",
    "        for column in self.dataset_backwards.columns:\n",
    "            if column == 'label':\n",
    "                continue\n",
    "            feature = self.feature_subset_backwards.drop(columns=[column]).to_numpy()\n",
    "            correct = 0\n",
    "            ### K-Fold CV\n",
    "            for cfv in range(int(len(feature)/self.k)):\n",
    "                train_data = np.delete(feature, cfv, axis=0)\n",
    "                test_data = feature[cfv]\n",
    "\n",
    "                knn = KNN_Classifier(train_data, test_data, 1)\n",
    "                knn.train()\n",
    "                if knn.pred_class()[0] == test_data[0]:\n",
    "                    correct += 1\n",
    "            precision.append(correct/int(len(feature)/self.k))\n",
    "            feature_name.append(column)\n",
    "            \n",
    "            if self.feature_set_backwards:\n",
    "                feat_str = ','.join(str(feature) for feature in self.feature_set_backwards)\n",
    "                print(\"\\tRemoving feature(s) {\" + feat_str + \",\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)*100) + '%')\n",
    "            else:\n",
    "                print(\"\\tRemoving feature(s) {\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)*100) + '%')\n",
    "\n",
    "        precision_index = np.argsort(precision)\n",
    "        self.feature_set_backwards.append(feature_name[precision_index[-1]])\n",
    "        feat_str = ','.join(str(feature) for feature in self.feature_set_backwards)\n",
    "        prev = self.best_features[1]\n",
    "        self.best_features[1] = max(self.best_features[1], precision[precision_index[-1]])\n",
    "        if (self.best_features[1] != prev):\n",
    "            self.best_features[0] = feat_str\n",
    "        if precision[precision_index[-1]] < self.best_features[1]:\n",
    "            print(\"\\n(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\")\n",
    "        print(\"\\nFeature set {\" + feat_str + \"} was best, accuracy is \" + str(precision[precision_index[-1]]*100) + \"%\\n\")\n",
    "        ### Referenced Pandas library to understand how to drop column https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "        self.feature_subset_backwards = self.feature_subset_backwards.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        self.dataset_backwards = self.dataset_backwards.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        self.greedy_backward()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to KNN Feature Selection Algorithm!\n",
      "\n",
      "Please select the file you would like to test: \n",
      "1. Sample Small 19\n",
      "2. Assigned Small 21\n",
      "3. Sample Large 6\n",
      "4. Assigned Large 13\n",
      "\n",
      "Please select the algorithm you would like to run: \n",
      "\n",
      "1. Forward Selection\n",
      "2. Backward Elimination\n",
      "3. Both\n",
      "\n",
      "This dataset has 12 features (not including the class attribute), with 500 instances.\n",
      "\n",
      "Running KNN with no features, I get an accuracy of 78.4%\n",
      "\n",
      "Beginning search.\n",
      "\n",
      "\tUsing feature(s) {1} accuracy is 67.2%\n",
      "\tUsing feature(s) {2} accuracy is 63.6%\n",
      "\tUsing feature(s) {3} accuracy is 65.0%\n",
      "\tUsing feature(s) {4} accuracy is 65.60000000000001%\n",
      "\tUsing feature(s) {5} accuracy is 64.2%\n",
      "\tUsing feature(s) {6} accuracy is 84.0%\n",
      "\tUsing feature(s) {7} accuracy is 65.4%\n",
      "\tUsing feature(s) {8} accuracy is 66.8%\n",
      "\tUsing feature(s) {9} accuracy is 74.2%\n",
      "\tUsing feature(s) {10} accuracy is 65.60000000000001%\n",
      "\tUsing feature(s) {11} accuracy is 65.8%\n",
      "\tUsing feature(s) {12} accuracy is 66.4%\n",
      "\n",
      "Feature set {6} was best, accuracy is 84.0%\n",
      "\n",
      "\tUsing feature(s) {6,1} accuracy is 82.8%\n",
      "\tUsing feature(s) {6,2} accuracy is 82.19999999999999%\n",
      "\tUsing feature(s) {6,3} accuracy is 81.6%\n",
      "\tUsing feature(s) {6,4} accuracy is 82.19999999999999%\n",
      "\tUsing feature(s) {6,5} accuracy is 83.6%\n",
      "\tUsing feature(s) {6,7} accuracy is 80.80000000000001%\n",
      "\tUsing feature(s) {6,8} accuracy is 80.80000000000001%\n",
      "\tUsing feature(s) {6,9} accuracy is 95.0%\n",
      "\tUsing feature(s) {6,10} accuracy is 81.2%\n",
      "\tUsing feature(s) {6,11} accuracy is 85.8%\n",
      "\tUsing feature(s) {6,12} accuracy is 81.39999999999999%\n",
      "\n",
      "Feature set {6,9} was best, accuracy is 95.0%\n",
      "\n",
      "\tUsing feature(s) {6,9,1} accuracy is 91.4%\n",
      "\tUsing feature(s) {6,9,2} accuracy is 92.4%\n",
      "\tUsing feature(s) {6,9,3} accuracy is 91.8%\n",
      "\tUsing feature(s) {6,9,4} accuracy is 92.80000000000001%\n",
      "\tUsing feature(s) {6,9,5} accuracy is 92.60000000000001%\n",
      "\tUsing feature(s) {6,9,7} accuracy is 92.80000000000001%\n",
      "\tUsing feature(s) {6,9,8} accuracy is 92.80000000000001%\n",
      "\tUsing feature(s) {6,9,10} accuracy is 92.2%\n",
      "\tUsing feature(s) {6,9,11} accuracy is 94.6%\n",
      "\tUsing feature(s) {6,9,12} accuracy is 91.60000000000001%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11} was best, accuracy is 94.6%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,1} accuracy is 87.6%\n",
      "\tUsing feature(s) {6,9,11,2} accuracy is 91.2%\n",
      "\tUsing feature(s) {6,9,11,3} accuracy is 88.8%\n",
      "\tUsing feature(s) {6,9,11,4} accuracy is 89.60000000000001%\n",
      "\tUsing feature(s) {6,9,11,5} accuracy is 91.0%\n",
      "\tUsing feature(s) {6,9,11,7} accuracy is 88.4%\n",
      "\tUsing feature(s) {6,9,11,8} accuracy is 90.8%\n",
      "\tUsing feature(s) {6,9,11,10} accuracy is 92.0%\n",
      "\tUsing feature(s) {6,9,11,12} accuracy is 87.4%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10} was best, accuracy is 92.0%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,1} accuracy is 86.0%\n",
      "\tUsing feature(s) {6,9,11,10,2} accuracy is 86.2%\n",
      "\tUsing feature(s) {6,9,11,10,3} accuracy is 86.0%\n",
      "\tUsing feature(s) {6,9,11,10,4} accuracy is 84.8%\n",
      "\tUsing feature(s) {6,9,11,10,5} accuracy is 84.8%\n",
      "\tUsing feature(s) {6,9,11,10,7} accuracy is 83.39999999999999%\n",
      "\tUsing feature(s) {6,9,11,10,8} accuracy is 84.39999999999999%\n",
      "\tUsing feature(s) {6,9,11,10,12} accuracy is 82.19999999999999%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2} was best, accuracy is 86.2%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1} accuracy is 85.39999999999999%\n",
      "\tUsing feature(s) {6,9,11,10,2,3} accuracy is 82.39999999999999%\n",
      "\tUsing feature(s) {6,9,11,10,2,4} accuracy is 80.4%\n",
      "\tUsing feature(s) {6,9,11,10,2,5} accuracy is 82.6%\n",
      "\tUsing feature(s) {6,9,11,10,2,7} accuracy is 79.80000000000001%\n",
      "\tUsing feature(s) {6,9,11,10,2,8} accuracy is 81.2%\n",
      "\tUsing feature(s) {6,9,11,10,2,12} accuracy is 80.2%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1} was best, accuracy is 85.39999999999999%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,3} accuracy is 79.80000000000001%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,4} accuracy is 78.4%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,5} accuracy is 79.60000000000001%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,7} accuracy is 80.4%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8} accuracy is 81.0%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,12} accuracy is 80.2%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8} was best, accuracy is 81.0%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,3} accuracy is 77.8%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,4} accuracy is 77.8%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,5} accuracy is 78.0%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,7} accuracy is 76.4%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12} accuracy is 78.0%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8,12} was best, accuracy is 78.0%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,3} accuracy is 74.4%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,4} accuracy is 74.2%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5} accuracy is 77.2%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,7} accuracy is 72.2%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8,12,5} was best, accuracy is 77.2%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,3} accuracy is 74.0%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,4} accuracy is 73.6%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,7} accuracy is 74.4%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8,12,5,7} was best, accuracy is 74.4%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,7,3} accuracy is 75.2%\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,7,4} accuracy is 73.0%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8,12,5,7,3} was best, accuracy is 75.2%\n",
      "\n",
      "\tUsing feature(s) {6,9,11,10,2,1,8,12,5,7,3,4} accuracy is 75.0%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {6,9,11,10,2,1,8,12,5,7,3,4} was best, accuracy is 75.0%\n",
      "\n",
      "Finished search!!! The best feature subset is {6,9}, which has an accuracy of 95.0%\n",
      "Elapsed (with compilation) = 31.997931003570557\n"
     ]
    }
   ],
   "source": [
    "# ### Start by splitting my data into train and test set\n",
    "# data = {'label': [2, 1, 2, 1, 2],\n",
    "#         'feature_1': [1, 10, 2, 11, 1],\n",
    "#         'feature_2': [ 2, 11, 3, 12, 2],\n",
    "#         'feature_3': [ 15, 15, 15, 15, 15],}\n",
    "# df = pd.DataFrame(data)\n",
    "# # print(df)\n",
    "# # df = small_data_19.head(5)\n",
    "# # df = small_data_19\n",
    "# # df = large_data_6\n",
    "# train_data = df.sample(frac = 0.8, random_state = 4)\n",
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# # print(train_data)\n",
    "# test_data = df.drop(train_data.index)\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "# # print(test_data)\n",
    "\n",
    "# Asks the user for puzzle_size\n",
    "print(\"Welcome to KNN Feature Selection Algorithm!\\n\")\n",
    "print(\"Please select the file you would like to test: \")\n",
    "print(\"1. Sample Small 19\\n\" + \n",
    "        \"2. Assigned Small 21\\n\" +\n",
    "        \"3. Sample Large 6\\n\" + \n",
    "        \"4. Assigned Large 13\\n\")\n",
    "file = input(\"Your selection: \")\n",
    "file = int(file)\n",
    "if file == 1:\n",
    "        df = small_data_19\n",
    "elif file == 2:\n",
    "        df = small_data_21\n",
    "elif file == 3:\n",
    "        df = large_data_6\n",
    "elif file == 4:\n",
    "        df = large_data_13\n",
    "    \n",
    "# Asks the user the algorithm they would like to use\n",
    "print(\"Please select the algorithm you would like to run: \\n\")\n",
    "print(\"1. Forward Selection\\n\" + \n",
    "        \"2. Backward Elimination\\n\" + \n",
    "        \"3. Both\\n\")\n",
    "algo = input(\"Your selection: \")\n",
    "algo = int(algo)\n",
    "\n",
    "print(\"This dataset has \" + str(df.shape[1]-1) + \" features (not including the class attribute), with \" + str(df.shape[0]) + \" instances.\\n\")\n",
    "correct = 0\n",
    "feature = df.to_numpy()\n",
    "\n",
    "if algo == 1:     \n",
    "        pred_classes = feature[:, 0]\n",
    "        classes, counts = np.unique(pred_classes, return_counts=True)\n",
    "        if counts[1] > counts[0]:\n",
    "                print(\"Running KNN with no features, I get an accuracy of \" + str(counts[1]/(counts[0]+counts[1])*100) + \"%\\n\")\n",
    "        else:\n",
    "                print(\"Running KNN with no features, I get an accuracy of \" + str(counts[0]/(counts[0]+counts[1])*100) + \"%\\n\")\n",
    "\n",
    "if algo == 2:     \n",
    "        for cfv in range(int(len(feature))):\n",
    "                train_data = np.delete(feature, cfv, axis=0)\n",
    "                test_data = feature[cfv]\n",
    "\n",
    "                knn = KNN_Classifier(train_data, test_data, 1)\n",
    "                knn.train()\n",
    "                if knn.pred_class()[0] == test_data[0]:\n",
    "                        correct += 1\n",
    "        print(\"Running KNN with all \" + str(df.shape[1]-1) + \" features, using \\\"leave-one-out\\\" evaluation, I get an accuracy of \" + str(correct/len(feature)*100)+ \"%\\n\")\n",
    "\n",
    "feat_select = Feature_Selection(df, 1)\n",
    "print(\"Beginning search.\\n\")\n",
    "if algo == 1:\n",
    "        start = time.time()\n",
    "        feat_select.greedy_forward()\n",
    "        feat_select.print_best_features()\n",
    "        end = time.time()\n",
    "        print(\"Elapsed (with compilation) = %s\" % (end - start))\n",
    "elif algo == 2:\n",
    "        start = time.time()\n",
    "        feat_select.greedy_backward()\n",
    "        feat_select.print_best_features()\n",
    "        end = time.time()\n",
    "        print(\"Elapsed (after compilation) = %s\" % (end - start))\n",
    "elif algo == 3:\n",
    "        start = time.time()\n",
    "        print(\"Starting with Forward Selection.\\n\")\n",
    "        feat_select.greedy_forward()\n",
    "        feat_select.print_best_features()\n",
    "        end = time.time()\n",
    "        print(\"Elapsed (after compilation) = %s\" % (end - start))\n",
    "        start = time.time()\n",
    "        print(\"Starting Backward Elimination.\\n\")\n",
    "        feat_select.greedy_backward()\n",
    "        feat_select.print_best_features()\n",
    "        end = time.time()\n",
    "        print(\"Elapsed (after compilation) = %s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10, 4, 12}\n"
     ]
    }
   ],
   "source": [
    "nums = set()\n",
    "for i in range(1,13):\n",
    "    nums.add(i)\n",
    "\n",
    "features = {7,2,5,9,1,3,8,11,6}\n",
    "\n",
    "difference = nums.difference(features)\n",
    "\n",
    "print(difference)  # Output: {1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
