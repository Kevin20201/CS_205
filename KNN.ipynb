{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_13204/3494703561.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  small_data_19 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__19.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_13204/3494703561.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  large_data_6 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__6.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_13204/3494703561.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  small_data_21 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__21.txt', sep='  ', header=None)\n",
      "/var/folders/6j/jnsrc7z51m96yrq49msyk2rh0000gn/T/ipykernel_13204/3494703561.py:19: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  large_data_13 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__13.txt', sep='  ', header=None)\n"
     ]
    }
   ],
   "source": [
    "### Sample Files\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "small_data_19 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__19.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "small_data_19 = small_data_19.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "large_data_6 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__6.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "large_data_6 = large_data_6.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Assigned Files\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "small_data_21 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_small_Data__21.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "small_data_21 = small_data_21.rename(columns={0 : \"label\"})\n",
    "\n",
    "### Referenced GeeksforGeeks to read in a txt file https://www.geeksforgeeks.org/how-to-read-space-delimited-files-in-pandas/#\n",
    "large_data_13 = pd.read_csv('/Users/kevintu/Documents/Python/CS205/CS_205/CS205_large_Data__13.txt', sep='  ', header=None)\n",
    "### Referenced pandas library documentation for renaming a column https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "large_data_13 = large_data_13.rename(columns={0 : \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifier:\n",
    "    def __init__(self, train_set=None, test_set=None, k=None, nearest_neighbors=None):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.k = k\n",
    "        self.nearest_neightbors = nearest_neighbors\n",
    "\n",
    "    def distance(self, nums):\n",
    "        sum = 0\n",
    "        for num in nums:\n",
    "            sum += num**2\n",
    "        return np.sqrt(sum)\n",
    "        \n",
    "    def pred_class(self):\n",
    "        pred_classes = []\n",
    "        for row in range(self.k):\n",
    "            # print(\"KNN: \", self.train_set[self.nearest_neighbors[row]])\n",
    "            # print(\"KNN: \", self.train_set[self.nearest_neighbors[row]][0])\n",
    "            pred_classes.append(self.train_set[self.nearest_neighbors[row]][0])\n",
    "        ### Referenced NumPy unique to return the predicted class https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
    "        classes, counts = np.unique(pred_classes, return_counts=True)\n",
    "        if len(counts) == 1:\n",
    "            return classes\n",
    "        if counts[0] > counts[1]:\n",
    "            return classes[0]\n",
    "        return classes[1]\n",
    "    \n",
    "    def train(self):\n",
    "        ### Test every testing entry to training entry\n",
    "        nearest_neighbors = []\n",
    "        # print(self.test_set)\n",
    "        for train_row in self.train_set:\n",
    "            # print(train_row)\n",
    "            # for test_row in self.test_set:\n",
    "            #     print(test_row)\n",
    "            nearest_neighbors.append(self.distance(train_row[1:] - self.test_set[1:]))\n",
    "        # print(nearest_neighbors)\n",
    "        ### Referenced NumPy argsort to sort by index of distances https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "        self.nearest_neighbors = np.argsort(nearest_neighbors)\n",
    "        # print(self.nearest_neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Start by splitting my data into train and test set\n",
    "data = {'label': [2.0000000e+000, 1.0000000e+000, 2.0000000e+000, 1.0000000e+000, 2],\n",
    "        'feature_1': [1.2340000e+010, 6.0668000e+000, 2.3400000e+010, 4.5645400e+010, 1],\n",
    "        'feature_2': [ 2.3440000e+000, 5.0770000e+000, 3.6460000e+000, 3.0045000e+000, 2]}\n",
    "data = {'label': [2, 1, 2, 1, 2],\n",
    "        'feature_1': [1, 10, 2, 11, 1],\n",
    "        'feature_2': [ 2, 11, 3, 12, 2]}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "train_data = df.sample(frac = 0.8, random_state = 4)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "# print(train_data)\n",
    "test_data = df.drop(train_data.index)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "# print(test_data)\n",
    "\n",
    "# knn = KNN_Classifier(train_data.to_numpy(), test_data.to_numpy(), 3)\n",
    "# knn.train()\n",
    "# print(knn.pred_class())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Selection:\n",
    "    def __init__(self, dataset=None, k=None):\n",
    "        self.dataset=dataset\n",
    "        self.dataset_backwards=copy.deepcopy(self.dataset)\n",
    "        self.k=k\n",
    "        self.feature_set = []\n",
    "        self.feature_set_backwards = []\n",
    "        self.feature_subset = pd.DataFrame(dataset['label'])\n",
    "        self.feature_subset_backwards = copy.deepcopy(self.dataset)\n",
    "        self.best_features = [-1, -1]\n",
    "    \n",
    "    def print_best_features(self):\n",
    "        print(\"Finished search!!! The best feature subset is {\" + self.best_features[0] + \"}, which has an accuracy of \" + str(self.best_features[1]) + \"%\")\n",
    "\n",
    "    def greedy_forward(self):\n",
    "        if self.dataset.shape[1] == 1:\n",
    "            return\n",
    "        precision = []\n",
    "        feature_name = []\n",
    "        ### Retreiving the Columns for Feature Selection\n",
    "        for column in self.dataset.columns:\n",
    "            if column == 'label':\n",
    "                continue\n",
    "            feature = pd.concat([self.feature_subset, pd.DataFrame(self.dataset[column])], axis=1).to_numpy()\n",
    "            # print(feature)\n",
    "            correct = 0\n",
    "            ### K-Fold CV\n",
    "            for cfv in range(int(len(feature)/self.k)):\n",
    "                # print(cfv)\n",
    "                # print(len(feature)/self.k)\n",
    "                train_data = np.delete(feature, cfv, axis=0)\n",
    "                # print(train_data)\n",
    "                test_data = feature[cfv]\n",
    "                # print(test_data)\n",
    "\n",
    "                knn = KNN_Classifier(train_data, test_data, 1)\n",
    "                knn.train()\n",
    "                # print(test_data[0])\n",
    "                # print(knn.pred_class()[0])\n",
    "                if knn.pred_class()[0] == test_data[0]:\n",
    "                    # print('HELLO')\n",
    "                    correct += 1\n",
    "            precision.append(correct/int(len(feature)/self.k))\n",
    "            feature_name.append(column)\n",
    "            if self.feature_set:\n",
    "                feat_str = ','.join(str(feature) for feature in self.feature_set)\n",
    "                print(\"\\tUsing feature(s) {\" + feat_str + \",\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)) + '%')\n",
    "            else:\n",
    "                print(\"\\tUsing feature(s) {\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)) + '%')\n",
    "        # print(precision)\n",
    "        # print(feature_name)\n",
    "        precision_index = np.argsort(precision)\n",
    "        # print(precision_index)\n",
    "        self.feature_set.append(feature_name[precision_index[-1]])\n",
    "        # print(precision[precision_index[-1]])\n",
    "        # print(self.feature_set)\n",
    "        feat_str = ','.join(str(feature) for feature in self.feature_set)\n",
    "        prev = self.best_features[1]\n",
    "        self.best_features[1] = max(self.best_features[1], precision[precision_index[-1]])\n",
    "        if (self.best_features[1] != prev):\n",
    "            self.best_features[0] = feat_str\n",
    "        if precision[precision_index[-1]] < self.best_features[1]:\n",
    "            print(\"\\n(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\")\n",
    "        print(\"\\nFeature set {\" + feat_str + \"} was best, accuracy is \" + str(precision[precision_index[-1]]) + \"%\\n\")\n",
    "        ### Referenced Pandas library to understand how to drop column https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "        self.feature_subset = pd.concat([self.feature_subset, pd.DataFrame(self.dataset[feature_name[precision_index[-1]]])], axis=1)\n",
    "        # print(self.feature_subset)\n",
    "        self.dataset = self.dataset.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        # print(self.dataset)\n",
    "        self.greedy_forward()\n",
    "        pass\n",
    "    \n",
    "    def greedy_backward(self):\n",
    "        if self.feature_subset_backwards.shape[1] == 1:\n",
    "            return\n",
    "        precision = []\n",
    "        feature_name = []\n",
    "        ### Retreiving the Columns for Feature Selection\n",
    "        for column in self.dataset_backwards.columns:\n",
    "            if column == 'label':\n",
    "                continue\n",
    "            # print(column)\n",
    "            feature = self.feature_subset_backwards.drop(columns=[column]).to_numpy()\n",
    "            # print(feature)\n",
    "            correct = 0\n",
    "            ### K-Fold CV\n",
    "            for cfv in range(int(len(feature)/self.k)):\n",
    "                # print(cfv)\n",
    "                # print(len(feature)/self.k)\n",
    "                train_data = np.delete(feature, cfv, axis=0)\n",
    "                # print(train_data)\n",
    "                test_data = feature[cfv]\n",
    "                # print(test_data)\n",
    "\n",
    "                knn = KNN_Classifier(train_data, test_data, 1)\n",
    "                knn.train()\n",
    "                # print(test_data[0])\n",
    "                # print(knn.pred_class()[0])\n",
    "                if knn.pred_class()[0] == test_data[0]:\n",
    "                    # print('HELLO')\n",
    "                    correct += 1\n",
    "            precision.append(correct/int(len(feature)/self.k))\n",
    "            feature_name.append(column)\n",
    "            if self.feature_set_backwards:\n",
    "                feat_str = ','.join(str(feature) for feature in self.feature_set_backwards)\n",
    "                print(\"\\tRemoving feature(s) {\" + feat_str + \",\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)) + '%')\n",
    "            else:\n",
    "                print(\"\\tRemoving feature(s) {\" + str(column) + \"} accuracy is \" + str(correct/int(len(feature)/self.k)) + '%')\n",
    "        # print(precision)\n",
    "        # print(feature_name)\n",
    "        precision_index = np.argsort(precision)\n",
    "        # print(precision_index)\n",
    "        # print(precision[precision_index[-1]])\n",
    "        self.feature_set_backwards.append(feature_name[precision_index[-1]])\n",
    "        # print(self.feature_set_backwards)\n",
    "        feat_str = ','.join(str(feature) for feature in self.feature_set_backwards)\n",
    "        prev = self.best_features[1]\n",
    "        self.best_features[1] = max(self.best_features[1], precision[precision_index[-1]])\n",
    "        if (self.best_features[1] != prev):\n",
    "            self.best_features[0] = feat_str\n",
    "        if precision[precision_index[-1]] < self.best_features[1]:\n",
    "            print(\"\\n(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\")\n",
    "        print(\"\\nFeature set {\" + feat_str + \"} was best, accuracy is \" + str(precision[precision_index[-1]]) + \"%\\n\")\n",
    "        ### Referenced Pandas library to understand how to drop column https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "        self.feature_subset_backwards = self.feature_subset_backwards.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        # print(self.feature_subset_backwards)\n",
    "        self.dataset_backwards = self.dataset_backwards.drop(columns=[feature_name[precision_index[-1]]])\n",
    "        # print(self.dataset_backwards)\n",
    "        self.greedy_backward()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to KNN Feature Selection Algorithm!\n",
      "\n",
      "Please select the file you would like to test: \n",
      "1. Sample Small 19\n",
      "2. Assigned Small 21\n",
      "3. Sample Large 6\n",
      "4. Assigned Larg 13e\n",
      "\n",
      "Please select the algorithm you would like to run: \n",
      "\n",
      "1. Forward Selection\n",
      "2. Backward Elimination\n",
      "3. Both\n",
      "\n",
      "This dataset has 12 features (not including the class attribute), with 500 instances.\n",
      "\n",
      "Running KNN with all 12 åfeatures, using \"leave-one-out\" evaluation, I get an accuracy of 0.75%\n",
      "\n",
      "Beginning search.\n",
      "\n",
      "\tRemoving feature(s) {1} accuracy is 0.734%\n",
      "\tRemoving feature(s) {2} accuracy is 0.708%\n",
      "\tRemoving feature(s) {3} accuracy is 0.73%\n",
      "\tRemoving feature(s) {4} accuracy is 0.752%\n",
      "\tRemoving feature(s) {5} accuracy is 0.726%\n",
      "\tRemoving feature(s) {6} accuracy is 0.672%\n",
      "\tRemoving feature(s) {7} accuracy is 0.714%\n",
      "\tRemoving feature(s) {8} accuracy is 0.734%\n",
      "\tRemoving feature(s) {9} accuracy is 0.702%\n",
      "\tRemoving feature(s) {10} accuracy is 0.736%\n",
      "\tRemoving feature(s) {11} accuracy is 0.758%\n",
      "\tRemoving feature(s) {12} accuracy is 0.75%\n",
      "\n",
      "Feature set {11} was best, accuracy is 0.758%\n",
      "\n",
      "\tRemoving feature(s) {11,1} accuracy is 0.76%\n",
      "\tRemoving feature(s) {11,2} accuracy is 0.734%\n",
      "\tRemoving feature(s) {11,3} accuracy is 0.766%\n",
      "\tRemoving feature(s) {11,4} accuracy is 0.774%\n",
      "\tRemoving feature(s) {11,5} accuracy is 0.784%\n",
      "\tRemoving feature(s) {11,6} accuracy is 0.7%\n",
      "\tRemoving feature(s) {11,7} accuracy is 0.746%\n",
      "\tRemoving feature(s) {11,8} accuracy is 0.768%\n",
      "\tRemoving feature(s) {11,9} accuracy is 0.71%\n",
      "\tRemoving feature(s) {11,10} accuracy is 0.762%\n",
      "\tRemoving feature(s) {11,12} accuracy is 0.77%\n",
      "\n",
      "Feature set {11,5} was best, accuracy is 0.784%\n",
      "\n",
      "\tRemoving feature(s) {11,5,1} accuracy is 0.744%\n",
      "\tRemoving feature(s) {11,5,2} accuracy is 0.77%\n",
      "\tRemoving feature(s) {11,5,3} accuracy is 0.778%\n",
      "\tRemoving feature(s) {11,5,4} accuracy is 0.784%\n",
      "\tRemoving feature(s) {11,5,6} accuracy is 0.698%\n",
      "\tRemoving feature(s) {11,5,7} accuracy is 0.76%\n",
      "\tRemoving feature(s) {11,5,8} accuracy is 0.758%\n",
      "\tRemoving feature(s) {11,5,9} accuracy is 0.722%\n",
      "\tRemoving feature(s) {11,5,10} accuracy is 0.762%\n",
      "\tRemoving feature(s) {11,5,12} accuracy is 0.784%\n",
      "\n",
      "Feature set {11,5,12} was best, accuracy is 0.784%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,1} accuracy is 0.774%\n",
      "\tRemoving feature(s) {11,5,12,2} accuracy is 0.798%\n",
      "\tRemoving feature(s) {11,5,12,3} accuracy is 0.77%\n",
      "\tRemoving feature(s) {11,5,12,4} accuracy is 0.79%\n",
      "\tRemoving feature(s) {11,5,12,6} accuracy is 0.656%\n",
      "\tRemoving feature(s) {11,5,12,7} accuracy is 0.784%\n",
      "\tRemoving feature(s) {11,5,12,8} accuracy is 0.79%\n",
      "\tRemoving feature(s) {11,5,12,9} accuracy is 0.722%\n",
      "\tRemoving feature(s) {11,5,12,10} accuracy is 0.788%\n",
      "\n",
      "Feature set {11,5,12,2} was best, accuracy is 0.798%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,1} accuracy is 0.814%\n",
      "\tRemoving feature(s) {11,5,12,2,3} accuracy is 0.782%\n",
      "\tRemoving feature(s) {11,5,12,2,4} accuracy is 0.81%\n",
      "\tRemoving feature(s) {11,5,12,2,6} accuracy is 0.698%\n",
      "\tRemoving feature(s) {11,5,12,2,7} accuracy is 0.82%\n",
      "\tRemoving feature(s) {11,5,12,2,8} accuracy is 0.826%\n",
      "\tRemoving feature(s) {11,5,12,2,9} accuracy is 0.742%\n",
      "\tRemoving feature(s) {11,5,12,2,10} accuracy is 0.81%\n",
      "\n",
      "Feature set {11,5,12,2,8} was best, accuracy is 0.826%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,1} accuracy is 0.816%\n",
      "\tRemoving feature(s) {11,5,12,2,8,3} accuracy is 0.81%\n",
      "\tRemoving feature(s) {11,5,12,2,8,4} accuracy is 0.842%\n",
      "\tRemoving feature(s) {11,5,12,2,8,6} accuracy is 0.674%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7} accuracy is 0.866%\n",
      "\tRemoving feature(s) {11,5,12,2,8,9} accuracy is 0.728%\n",
      "\tRemoving feature(s) {11,5,12,2,8,10} accuracy is 0.848%\n",
      "\n",
      "Feature set {11,5,12,2,8,7} was best, accuracy is 0.866%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,1} accuracy is 0.846%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,3} accuracy is 0.814%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,4} accuracy is 0.862%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,6} accuracy is 0.696%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,9} accuracy is 0.76%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10} accuracy is 0.866%\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10} was best, accuracy is 0.866%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,1} accuracy is 0.89%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3} accuracy is 0.892%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,4} accuracy is 0.882%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,6} accuracy is 0.73%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,9} accuracy is 0.776%\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10,3} was best, accuracy is 0.892%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1} accuracy is 0.928%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,4} accuracy is 0.914%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,6} accuracy is 0.706%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,9} accuracy is 0.82%\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10,3,1} was best, accuracy is 0.928%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,4} accuracy is 0.95%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,6} accuracy is 0.74%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,9} accuracy is 0.822%\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10,3,1,4} was best, accuracy is 0.95%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,4,6} accuracy is 0.742%\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,4,9} accuracy is 0.84%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10,3,1,4,9} was best, accuracy is 0.84%\n",
      "\n",
      "\tRemoving feature(s) {11,5,12,2,8,7,10,3,1,4,9,6} accuracy is 0.214%\n",
      "\n",
      "(Warning, Accuracy has decreased!!! Continuing search in case of local maxima)\n",
      "\n",
      "Feature set {11,5,12,2,8,7,10,3,1,4,9,6} was best, accuracy is 0.214%\n",
      "\n",
      "Finished search!!! The best feature subset is {11,5,12,2,8,7,10,3,1,4}, which has an accuracy of 0.95%\n"
     ]
    }
   ],
   "source": [
    "### Start by splitting my data into train and test set\n",
    "data = {'label': [2, 1, 2, 1, 2],\n",
    "        'feature_1': [1, 10, 2, 11, 1],\n",
    "        'feature_2': [ 2, 11, 3, 12, 2],\n",
    "        'feature_3': [ 15, 15, 15, 15, 15],}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df)\n",
    "# df = small_data_19.head(5)\n",
    "# df = small_data_19\n",
    "# df = large_data_6\n",
    "train_data = df.sample(frac = 0.8, random_state = 4)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "# print(train_data)\n",
    "test_data = df.drop(train_data.index)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "# print(test_data)\n",
    "\n",
    "# Asks the user for puzzle_size\n",
    "print(\"Welcome to KNN Feature Selection Algorithm!\\n\")\n",
    "print(\"Please select the file you would like to test: \")\n",
    "print(\"1. Sample Small 19\\n\" + \n",
    "        \"2. Assigned Small 21\\n\" +\n",
    "        \"3. Sample Large 6\\n\" + \n",
    "        \"4. Assigned Larg 13e\\n\")\n",
    "file = input(\"Your selection: \")\n",
    "file = int(file)\n",
    "if file == 1:\n",
    "        df = small_data_19\n",
    "elif file == 2:\n",
    "        df = small_data_21\n",
    "elif file == 3:\n",
    "        df = large_data_6\n",
    "elif file == 4:\n",
    "        df = large_data_13\n",
    "    \n",
    "# Asks the user the algorithm they would like to use\n",
    "print(\"Please select the algorithm you would like to run: \\n\")\n",
    "print(\"1. Forward Selection\\n\" + \n",
    "        \"2. Backward Elimination\\n\" + \n",
    "        \"3. Both\\n\")\n",
    "algo = input(\"Your selection: \")\n",
    "algo = int(algo)\n",
    "\n",
    "print(\"This dataset has \" + str(df.shape[1]-1) + \" features (not including the class attribute), with \" + str(df.shape[0]) + \" instances.\\n\")\n",
    "correct = 0\n",
    "feature = df.to_numpy()\n",
    "for cfv in range(int((df.shape[1]-1)/1)):\n",
    "        train_data = np.delete(feature, cfv, axis=0)\n",
    "        test_data = feature[cfv]\n",
    "\n",
    "        knn = KNN_Classifier(train_data, test_data, 1)\n",
    "        knn.train()\n",
    "        if knn.pred_class()[0] == test_data[0]:\n",
    "                correct += 1\n",
    "print(\"Running KNN with all \" + str(df.shape[1]-1) + \" åfeatures, using \\\"leave-one-out\\\" evaluation, I get an accuracy of \" + str(correct/int((df.shape[1]-1)/1)) + \"%\\n\")\n",
    "\n",
    "feat_select = Feature_Selection(df, 1)\n",
    "print(\"Beginning search.\\n\")\n",
    "if algo == 1:\n",
    "        feat_select.greedy_forward()\n",
    "        feat_select.print_best_features()\n",
    "elif algo == 2:\n",
    "        feat_select.greedy_backward()\n",
    "        feat_select.print_best_features()\n",
    "elif algo == 3:\n",
    "        print(\"Starting with Forward Selection.\\n\")\n",
    "        feat_select.greedy_forward()\n",
    "        feat_select.print_best_features()\n",
    "        print(\"Starting Backward Elimination.\\n\")\n",
    "        feat_select.greedy_backward()\n",
    "        feat_select.print_best_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
